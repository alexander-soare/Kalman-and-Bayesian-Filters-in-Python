{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter is very basic recap of the Gaussian function. Worth remind myself of is the product and sum of Guassians.\n",
    "\n",
    "The Guassian probability distribution with mean $\\mu$ and variance $\\sigma^2$ is:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^\\frac{(x-\\mu)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "Then the product of two Gaussians gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &=\\frac{\\sigma_1^2\\mu_2 + \\sigma_2^2\\mu_1}{\\sigma_1^2+\\sigma_2^2}\\\\\n",
    "\\sigma^2 &=\\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be derived by simply multiplying formulas for the gaussian and then chugging through the algebra. One important trick to remember is that a constant in the exponent amounts to a constant multiplier which gets normalized away anyway, so you can make it whatever you want to complete the square. This rule can then be used in the $likelihood \\times prior$ step of the Guassian filter update. The idea is that rather than doing element-wise multiplication of potentially huge, potentionally multidimensional distributions, you just need to multiply Gaussians.\n",
    "\n",
    "The Guassian representing the sum of two Gaussian variables gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &= \\mu_1 + \\mu_2 \\\\\n",
    "\\sigma^2 &= \\sigma_1^2 + \\sigma_2^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "There are proofs on the [Wikipedia page](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables). The one reference in the book is the proof using convolutions (although in the book I think there are typos in the proof). This rule can be used in the propagation step from the train example in chapter 2 where we convolve the posterior with the propagation operator. Let me just think about it here. So for independent random variables $X$ and $Y$, the distribution $f_Z$ of $Z = X + Y$ is\n",
    "\n",
    "$$\n",
    "f_Z(Z) = \\int_{-\\infty}^{\\infty} f_Y(z-x)f_X(x) dx\n",
    "$$\n",
    "\n",
    "What we're doing here is we're integrating over all possible pairs of positions where the $x$-axis sums to $z$. Mentally take two zero-centered gaussians, one skinny, one fat. Take the skinny one and move it to the left by z, then flip the result about the y axis. Now anywhere you pick, the sum of the random variables is z. By integrating from $-\\infty$ to $\\infty$ you're taking like a weighted sum of one pdf in terms of the other at all places where their random variable adds to $z$. There are infinite such places. Just think of $z=5$ for instance. There's $-2 + 7$, $-1 + 6$, $0 + 5$, $1 + 4$, $2 + 3$ etc. Notice how they move in opposite direcitons (hence the flip). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a piece about Bayes' theorem. Again, this is just a basic recap for me, but one part is useful to remind myself of. It's much easier to know $p(z|x)$ than $p(x|z)$ in practice. That is, it's much easier to know what the measurement might look like given the state. That's because we can prepare a bunch of samples in known states and make measurements. But it's much harder to know the state given the measurement, because we can't actually prepare experiments for this. That's why Bayes' theorem is super helpful. It lets use compute $p(x|z)$ given $p(z|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one last bit that I should not forget in future is that generally we use $P(X=x)$ as the \"probablity that random variable $X$ takes value $x$\" and $p(x)$ as the value of the probability distribution at $x$. These are different concepts and are not equal to one another  in the case of a continuous probablity distribution (where the former is 0 and the latter is not necessarily 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
